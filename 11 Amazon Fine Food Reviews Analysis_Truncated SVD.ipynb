{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Reviews Analysis\n",
    "\n",
    "\n",
    "Data Source: https://www.kaggle.com/snap/amazon-fine-food-reviews <br>\n",
    "\n",
    "EDA: https://nycdatascience.com/blog/student-works/amazon-fine-foods-visualization/\n",
    "\n",
    "\n",
    "The Amazon Fine Food Reviews dataset consists of reviews of fine foods from Amazon.<br>\n",
    "\n",
    "Number of reviews: 568,454<br>\n",
    "Number of users: 256,059<br>\n",
    "Number of products: 74,258<br>\n",
    "Timespan: Oct 1999 - Oct 2012<br>\n",
    "Number of Attributes/Columns in data: 10 \n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. Id\n",
    "2. ProductId - unique identifier for the product\n",
    "3. UserId - unqiue identifier for the user\n",
    "4. ProfileName\n",
    "5. HelpfulnessNumerator - number of users who found the review helpful\n",
    "6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n",
    "7. Score - rating between 1 and 5\n",
    "8. Time - timestamp for the review\n",
    "9. Summary - brief summary of the review\n",
    "10. Text - text of the review\n",
    "\n",
    "\n",
    "#### Objective:\n",
    "Given a review, determine whether the review is positive (rating of 4 or 5) or negative (rating of 1 or 2).\n",
    "\n",
    "<br>\n",
    "[Q] How to determine if a review is positive or negative?<br>\n",
    "<br> \n",
    "[Ans] We could use Score/Rating. A rating of 4 or 5 can be cosnidered as a positive review. A rating of 1 or 2 can be considered as negative one. A review of rating 3 is considered nuetral and such reviews are ignored from our analysis. This is an approximate and proxy way of determining the polarity (positivity/negativity) of a review.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1]. Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1.1] Loading the data\n",
    "\n",
    "The dataset is available in two forms\n",
    "1. .csv file\n",
    "2. SQLite Database\n",
    "\n",
    "In order to load the data, We have used the SQLITE dataset as it is easier to query the data and visualise the data efficiently.\n",
    "<br> \n",
    "\n",
    "Here as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score is above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cyborg\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in our data (1000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using SQLite Table to read data.\n",
    "# con = sqlite3.connect('database.sqlite') \n",
    "con = sqlite3.connect(r'C:\\Users\\Cyborg\\Applied AI\\assignment\\Assignments_AFR_2018\\database.sqlite') \n",
    "\n",
    "\n",
    "# filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3\n",
    "# SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000, will give top 500000 data points\n",
    "# you can change the number to any other number based on your computing power\n",
    "\n",
    "# filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 LIMIT 500000\"\"\", con) \n",
    "# for tsne assignment you can take 5k data points\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 LIMIT 1000\"\"\", con) \n",
    "\n",
    "# Give reviews with Score>3 a positive rating(1), and reviews with a score<3 a negative rating(0).\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative\n",
    "print(\"Number of data points in our data\", filtered_data.shape)\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  [2] Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2.1] Data Cleaning: Deduplication\n",
    "\n",
    "It is observed (as shown in the table below) that the reviews data had many duplicate entries. Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data.  Following is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "sorted_data=sorted_data.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(998, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking to see how much % of data still remains\n",
    "(final['Id'].size*1.0)/(filtered_data['Id'].size*1.0)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Observation:-</b> It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(998, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    841\n",
       "0    157\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Before starting the next phase of preprocessing lets see the number of entries left\n",
    "print(final.shape)\n",
    "\n",
    "#How many positive and negative reviews are present in our dataset?\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  [3] Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3.1].  Preprocessing Review Text\n",
    "\n",
    "Now that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n",
    "\n",
    "Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    "1. Begin by removing the html tags\n",
    "2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "5. Convert the word to lowercase\n",
    "6. Remove Stopwords\n",
    "7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n",
    "\n",
    "After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "# <br /><br /> ==> after the above steps, we are getting \"br br\"\n",
    "# we are including them into stop words list\n",
    "# instead of <br /> if we have <br/> these tags would have revmoved in the 1st step\n",
    "\n",
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 998/998 [00:00<00:00, 1352.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(final['Text'].values):\n",
    "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews.append(sentance.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'>[3.2] Preprocessing Review Summary</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5] Assignment 11: Truncated SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li><strong>Apply Truncated-SVD on only this feature set:</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>SET 2:</font>Review text, preprocessed one converted into vectors using (TFIDF)</li>\n",
    "    <br>\n",
    "    <li><strong>Procedure:</strong>\n",
    "        <ul>\n",
    "    <li>Take top 2000 or 3000 features from tf-idf vectorizers using idf_ score.</li>\n",
    "    <li>You need to calculate the co-occurrence matrix with the selected features (Note: X.X^T\n",
    "doesn’t give the co-occurrence matrix, it returns the covariance matrix, check these\n",
    "        bolgs <a href='https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285'>blog-1,</a> <a href='https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/'>blog-2 </a>for more information)</li>\n",
    "            <li>You should choose the n_components in truncated svd, with maximum explained\n",
    "variance. Please search on how to choose that and implement them. (hint: plot of\n",
    "cumulative explained variance ratio)</li>\n",
    "            <li>After you are done with the truncated svd, you can apply K-Means clustering and choose\n",
    "the best number of clusters based on elbow method.</li>\n",
    "            <li> Print out wordclouds for each cluster, similar to that in previous assignment. </li>\n",
    "            <li>You need to write a function that takes a word and returns the most similar words using\n",
    "cosine similarity between the vectors(vector: a row in the matrix after truncatedSVD)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <br>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncated-SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5.1] Taking top features from TFIDF,<font color='red'> SET 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write all the code with proper documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import plotly.offline as offline\n",
    "from sklearn.decomposition import TruncatedSVD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=preprocessed_reviews[:]\n",
    "y=final['Score'][:]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.30, random_state=42)\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf=tfidf.fit_transform(X_train)\n",
    "X_test_tfidf=tfidf.transform(X_test)\n",
    "tfidf_features=tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_value=\n",
    "imp_features=pd.DataFrame(tfidf.vocabulary_, index=tfidf_features)\n",
    "\n",
    "# dict(zip(tfidf.vocabulary_ , tfidf.idf_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4814, 4814)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>great</th>\n",
       "      <th>tasting</th>\n",
       "      <th>tea</th>\n",
       "      <th>looking</th>\n",
       "      <th>good</th>\n",
       "      <th>sassafras</th>\n",
       "      <th>forever</th>\n",
       "      <th>finally</th>\n",
       "      <th>ordering</th>\n",
       "      <th>product</th>\n",
       "      <th>...</th>\n",
       "      <th>extensive</th>\n",
       "      <th>infant</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>unfortnately</th>\n",
       "      <th>rock</th>\n",
       "      <th>appetizing</th>\n",
       "      <th>address</th>\n",
       "      <th>enhance</th>\n",
       "      <th>bummer</th>\n",
       "      <th>haters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abby</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>able</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absence</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolutely</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absorbed</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absorbs</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abt</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acai</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accept</th>\n",
       "      <td>1855</td>\n",
       "      <td>4281</td>\n",
       "      <td>4288</td>\n",
       "      <td>2466</td>\n",
       "      <td>1819</td>\n",
       "      <td>3654</td>\n",
       "      <td>1659</td>\n",
       "      <td>1585</td>\n",
       "      <td>2908</td>\n",
       "      <td>3276</td>\n",
       "      <td>...</td>\n",
       "      <td>1481</td>\n",
       "      <td>2145</td>\n",
       "      <td>2540</td>\n",
       "      <td>4519</td>\n",
       "      <td>3568</td>\n",
       "      <td>191</td>\n",
       "      <td>51</td>\n",
       "      <td>1380</td>\n",
       "      <td>530</td>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 4814 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            great  tasting   tea  looking  good  sassafras  forever  finally  \\\n",
       "abby         1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "able         1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "absence      1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "absolute     1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "absolutely   1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "absorbed     1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "absorbs      1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "abt          1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "acai         1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "accept       1855     4281  4288     2466  1819       3654     1659     1585   \n",
       "\n",
       "            ordering  product   ...    extensive  infant  manufacturer  \\\n",
       "abby            2908     3276   ...         1481    2145          2540   \n",
       "able            2908     3276   ...         1481    2145          2540   \n",
       "absence         2908     3276   ...         1481    2145          2540   \n",
       "absolute        2908     3276   ...         1481    2145          2540   \n",
       "absolutely      2908     3276   ...         1481    2145          2540   \n",
       "absorbed        2908     3276   ...         1481    2145          2540   \n",
       "absorbs         2908     3276   ...         1481    2145          2540   \n",
       "abt             2908     3276   ...         1481    2145          2540   \n",
       "acai            2908     3276   ...         1481    2145          2540   \n",
       "accept          2908     3276   ...         1481    2145          2540   \n",
       "\n",
       "            unfortnately  rock  appetizing  address  enhance  bummer  haters  \n",
       "abby                4519  3568         191       51     1380     530    1942  \n",
       "able                4519  3568         191       51     1380     530    1942  \n",
       "absence             4519  3568         191       51     1380     530    1942  \n",
       "absolute            4519  3568         191       51     1380     530    1942  \n",
       "absolutely          4519  3568         191       51     1380     530    1942  \n",
       "absorbed            4519  3568         191       51     1380     530    1942  \n",
       "absorbs             4519  3568         191       51     1380     530    1942  \n",
       "abt                 4519  3568         191       51     1380     530    1942  \n",
       "acai                4519  3568         191       51     1380     530    1942  \n",
       "accept              4519  3568         191       51     1380     530    1942  \n",
       "\n",
       "[10 rows x 4814 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5.2] Calulation of Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=np.argsort(tfidf.idf_[::-1])\n",
    "top_features=[tfidf_features[i] for i in indices[0:2000]]\n",
    "top_n=np.argsort(top_features[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 595, 1018, 1012,  176, 1676, 1183,  171,  154,   67], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_values=np.arange(2,150,3)\n",
    "top_n[:9] #just printing for self reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/PushpendraSinghChauhan/Amazon-Fine-Food-Reviews/blob/master/Computing%20Word%20Vectors%20using%20TruncatedSVD.ipynb\n",
    "def cooccurrenceMatrix(neighbour_num , list_words):\n",
    "        \n",
    "        # Storing all words with their indices in the dictionary\n",
    "        corpus = dict()\n",
    "        # List of all words in the corpus\n",
    "        doc = []\n",
    "        index = 0\n",
    "        for sent in preprocessed_reviews:\n",
    "            for word in sent.split():\n",
    "                doc.append(word)\n",
    "                corpus.setdefault(word,[])\n",
    "                corpus[word].append(index)\n",
    "                index += 1\n",
    "        \n",
    "        # Co-occurrence matrix\n",
    "        matrix = []\n",
    "        # rows in co-occurrence matrix\n",
    "        for row in list_words:\n",
    "            # row in co-occurrence matrix\n",
    "            temp = []\n",
    "            # column in co-occurrence matrix \n",
    "            for col in list_words :\n",
    "                if( col != row):\n",
    "                    # No. of times col word is in neighbourhood of row word\n",
    "                    count = 0\n",
    "                    # Value of neighbourhood\n",
    "                    num = neighbour_num\n",
    "                    # Indices of row word in the corpus\n",
    "                    positions = corpus[row]\n",
    "                    for i in positions:\n",
    "                        if i<(num-1):\n",
    "                            # Checking for col word in neighbourhood of row\n",
    "                            if col in doc[i:i+num]:\n",
    "                                count +=1\n",
    "                        elif (i>=(num-1)) and (i<=(len(doc)-num)):\n",
    "                            # Check col word in neighbour of row\n",
    "                            if (col in doc[i-(num-1):i+1]) and (col in doc[i:i+num]):\n",
    "                                count +=2\n",
    "                            # Check col word in neighbour of row\n",
    "                            elif (col in doc[i-(num-1):i+1]) or (col in doc[i:i+num]):\n",
    "                                count +=1\n",
    "                        else :\n",
    "                            if (col in doc[i-(num-1):i+1]):\n",
    "                                count +=1\n",
    "                            \n",
    "                                \n",
    "                    # appending the col count to row of co-occurrence matrix\n",
    "                    temp.append(count)\n",
    "                else:\n",
    "                    # Append 0 in the column if row and col words are equal\n",
    "                    temp.append(0)\n",
    "            # appending the row in co-occurrence matrix        \n",
    "            matrix.append(temp)\n",
    "        # Return co-occurrence matrix\n",
    "        return np.array(matrix)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cooccurrenceMatrix(5, top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35909"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write all the code with proper documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5.3] Finding optimal value for number of components (n) to be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write all the code with proper documentation\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance=[]\n",
    "# X=sparse.csr_matrix(top_features)\n",
    "for k in k_values:\n",
    "    svd=TruncatedSVD(n_components=k)\n",
    "    svd.fit_transform(X)\n",
    "\n",
    "    score=svd.explained_variance_ratio_.sum()\n",
    "    variance.append(score)\n",
    "plt.plot(k_values, variance)\n",
    "plt.xlabel('K Values')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5.4] Applying k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write all the code with proper documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd=TruncatedSVD(n_components=60)\n",
    "svd.fit(X)\n",
    "score=svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5.5] Wordclouds of clusters obtained in the above section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write all the code with proper documentation\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# w is the weight after fitting the model\n",
    "feature_importances = pd.DataFrame(top_n,index = top_features, columns=['importance']).sort_values('importance',ascending=False)\n",
    "a=feature_importances.iloc[0:20]\n",
    "comment_words = ' '\n",
    "for val in a.index: \n",
    "    val = str(val) \n",
    "    tokens = val.split() \n",
    "      \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "          \n",
    "    for words in tokens: \n",
    "        comment_words = comment_words + words + ' '\n",
    "\n",
    "        stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                stopwords = stopwords, \n",
    "                min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5.6] Function that returns most similar words for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write all the code with proper documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [6] Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write down few lines about what you observed from this assignment. \n",
    "# Also please do mention the optimal values that you obtained for number of components & number of clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
